{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mao/.virtualenvs/tf_quantization/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 as Net\n",
    "\n",
    "model = Net(weights='imagenet')\n",
    "\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "# Save the h5 file to path specified.\n",
    "model.save(\"./model/mobilenetv2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mao/.virtualenvs/tf_quantization/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mao/.virtualenvs/tf_quantization/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mao/.virtualenvs/tf_quantization/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "['input_1'] ['Logits/Softmax']\n",
      "WARNING:tensorflow:From <ipython-input-2-4f73380c46f7>:14: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.remove_training_nodes`\n",
      "WARNING:tensorflow:From <ipython-input-2-4f73380c46f7>:15: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/mao/.virtualenvs/tf_quantization/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 262 variables.\n",
      "INFO:tensorflow:Converted 262 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "\n",
    "# Clear any previous session.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "save_pb_dir = './model'\n",
    "model_fname = './model/mobilenetv2.h5'\n",
    "def freeze_graph(graph, session, output, save_pb_dir='.', save_pb_name='fp32_frozen_graph.pb', save_pb_as_text=False):\n",
    "    with graph.as_default():\n",
    "        graphdef_inf = tf.graph_util.remove_training_nodes(graph.as_graph_def())\n",
    "        graphdef_frozen = tf.graph_util.convert_variables_to_constants(session, graphdef_inf, output)\n",
    "        graph_io.write_graph(graphdef_frozen, save_pb_dir, save_pb_name, as_text=save_pb_as_text)\n",
    "        return graphdef_frozen\n",
    "\n",
    "# This line must be executed before loading Keras model.\n",
    "tf.keras.backend.set_learning_phase(0) \n",
    "\n",
    "model = load_model(model_fname)\n",
    "\n",
    "session = tf.keras.backend.get_session()\n",
    "\n",
    "input_names = [t.op.name for t in model.inputs]\n",
    "output_names = [t.op.name for t in model.outputs]\n",
    "\n",
    "# Prints input and output nodes names, take notes of them.\n",
    "print(input_names, output_names)\n",
    "\n",
    "fp32_frozen_graph = freeze_graph(session.graph, session, [out.op.name for out in model.outputs], save_pb_dir=save_pb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13974588"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "graph_def_file = \"/home/mao/Github/tensorRT/model/fp32_frozen_graph.pb\"\n",
    "input_arrays = [\"input_1\"]\n",
    "output_arrays = ['Logits/Softmax']\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_frozen_graph(\n",
    "  graph_def_file, input_arrays, output_arrays)\n",
    "tflite_model = converter.convert()\n",
    "open(\"./model/ .tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "# Optional image to test model prediction.\n",
    "img_path = './data/elephant.jpg'\n",
    "\n",
    "image_size = [224, 224, 3]\n",
    "img = image.load_img(img_path, target_size=image_size[:2])\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3) (1, 224, 224, 3)\n",
      "(1, 224, 224, 3) (1, 224, 224, 3)\n",
      "Calibration data shape:  (2, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 2\n",
    "BATCH_SIZE = 1\n",
    "batched_input = np.zeros((BATCH_SIZE * num_calibration_batches, 224, 224, 3), dtype=np.float32)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "    # prepare dataset iterator\n",
    "    next_element = tf.convert_to_tensor(x)\n",
    "    for i in range(num_calibration_batches):\n",
    "        print(batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :].shape, sess.run(next_element).shape)\n",
    "        batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :] = sess.run(next_element)\n",
    "\n",
    "#batched_input = tf.constant(batched_input)\n",
    "print('Calibration data shape: ', batched_input.shape)\n",
    "\n",
    "def calibration_input_fn_gen():\n",
    "    for i in range(num_calibration_batches):\n",
    "        yield [batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :]]\n",
    "        \n",
    "calibration_input_fn = calibration_input_fn_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3980480"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "graph_def_file = \"/home/mao/Github/tensorRT/model/fp32_frozen_graph.pb\"\n",
    "input_arrays = [\"input_1\"]\n",
    "output_arrays = ['Logits/Softmax']\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_frozen_graph(\n",
    "  graph_def_file, input_arrays, output_arrays)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = calibration_input_fn_gen\n",
    "tflite_model = converter.convert()\n",
    "open(\"./model/int8_frozen_graph.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfLiteInference(input_details, interpreter, output_details):\n",
    "    interpreter.set_tensor(input_details[0]['index'], x)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"./model/fp32_frozen_graph.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Optional image to test model prediction.\n",
    "img_path = './data/elephant.jpg'\n",
    "\n",
    "image_size = [224, 224, 3]\n",
    "img = image.load_img(img_path, target_size=image_size[:2])\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02504458', 'African_elephant', 0.5241098), ('n01871265', 'tusker', 0.17159039), ('n02504013', 'Indian_elephant', 0.15654095)]\n"
     ]
    }
   ],
   "source": [
    "output_data = tfLiteInference(input_details, interpreter, output_details)\n",
    "\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(output_data, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02504458', 'African_elephant', 0.5241098), ('n01871265', 'tusker', 0.17159039), ('n02504013', 'Indian_elephant', 0.15654095)]\n",
      "average(sec):0.02,fps:65.31\n"
     ]
    }
   ],
   "source": [
    "# test FP32 CPU\n",
    "import time\n",
    "times = []\n",
    "\n",
    "output_data = tfLiteInference(input_details, interpreter, output_details)\n",
    "\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(output_data, top=3)[0])\n",
    "\n",
    "for i in range(2000):\n",
    "    start_time = time.time()\n",
    "    output_data = tfLiteInference(input_details, interpreter, output_details)\n",
    "    delta = (time.time() - start_time)\n",
    "    times.append(delta)\n",
    "mean_delta = np.array(times).mean()\n",
    "fps = 1 / mean_delta\n",
    "print('average(sec):{:.2f},fps:{:.2f}'.format(mean_delta, fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02504458', 'African_elephant', 0.5859375), ('n01871265', 'tusker', 0.16015625), ('n02504013', 'Indian_elephant', 0.078125)]\n",
      "average(sec):0.93,fps:1.08\n"
     ]
    }
   ],
   "source": [
    "# test INT8 CPU\n",
    "import time\n",
    "times = []\n",
    "\n",
    "output_data = tfLiteInference(input_details, interpreter, output_details)\n",
    "\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(output_data, top=3)[0])\n",
    "\n",
    "for i in range(10):\n",
    "    start_time = time.time()\n",
    "    output_data = tfLiteInference(input_details, interpreter, output_details)\n",
    "    delta = (time.time() - start_time)\n",
    "    times.append(delta)\n",
    "mean_delta = np.array(times).mean()\n",
    "fps = 1 / mean_delta\n",
    "print('average(sec):{:.2f},fps:{:.2f}'.format(mean_delta, fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_and_bn(conv, bn):\n",
    "    #\n",
    "    # init\n",
    "    fusedconv = torch.nn.Conv2d(\n",
    "        conv.in_channels,\n",
    "        conv.out_channels,\n",
    "        kernel_size=conv.kernel_size,\n",
    "        stride=conv.stride,\n",
    "        padding=conv.padding,\n",
    "        bias=True\n",
    "    )\n",
    "    #\n",
    "    # prepare filters\n",
    "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))\n",
    "    fusedconv.weight.copy_( torch.mm(w_bn, w_conv).view(fusedconv.weight.size()) )\n",
    "    #\n",
    "    # prepare spatial bias\n",
    "    if conv.bias is not None:\n",
    "        b_conv = conv.bias\n",
    "    else:\n",
    "        b_conv = torch.zeros( conv.weight.size(0) )\n",
    "    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "    fusedconv.bias.copy_( b_conv + b_bn )\n",
    "    #\n",
    "    # we're done\n",
    "    return fusedconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.00000030\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "torch.set_grad_enabled(False)\n",
    "x = torch.randn(16, 3, 256, 256)\n",
    "rn18 = torchvision.models.resnet18(pretrained=True)\n",
    "rn18.eval()\n",
    "net = torch.nn.Sequential(\n",
    "    rn18.conv1,\n",
    "    rn18.bn1\n",
    ")\n",
    "y1 = net.forward(x)\n",
    "fusedconv = fuse_conv_and_bn(net[0], net[1])\n",
    "y2 = fusedconv.forward(x)\n",
    "d = (y1 - y2).norm().div(y1.norm()).item()\n",
    "print(\"error: %.8f\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
